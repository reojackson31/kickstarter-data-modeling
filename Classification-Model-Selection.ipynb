{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pycountry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Kickstarter.xlsx\")\n",
    "grading_df = pd.read_excel(\"Kickstarter-Grading-Sample.xlsx\")\n",
    "\n",
    "\n",
    "\n",
    "#Function to convert country codes to country name\n",
    "def get_country_name(code):\n",
    "    try:\n",
    "        return pycountry.countries.get(alpha_2=code).name\n",
    "    except AttributeError:\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n",
    "#Function for Pre-Processing Data for training data and grading data\n",
    "\n",
    "#Function for Pre-Processing Data for training data and grading data\n",
    "\n",
    "#1. Drop Columns that have no impact on prediction\n",
    "df = df.drop(columns=['id','name'], axis=1)\n",
    "\n",
    "#2. Drop Columns that can only be determined after knowing the state of the project\n",
    "cols_unknown_at_launch = columns_to_drop = ['pledged', 'state_changed_at', 'backers_count', 'usd_pledged',\n",
    "                   'state_changed_at_weekday', 'state_changed_at_month',\n",
    "                   'state_changed_at_day', 'state_changed_at_yr',\n",
    "                   'state_changed_at_hr', 'launch_to_state_change_days', 'spotlight', 'staff_pick']\n",
    "\n",
    "df = df.drop(columns=cols_unknown_at_launch, axis=1)\n",
    "\n",
    "#3. Drop Columns with repeated information in other columns:\n",
    "cols_repeat_info = ['currency', 'deadline', 'created_at', 'launched_at']\n",
    "df = df.drop(columns=cols_repeat_info, axis=1)\n",
    "\n",
    "#4. Remove rows with state other than successful or failed\n",
    "df = df[df['state'].isin(['successful', 'failed'])]\n",
    "df['state'] = df['state'].apply(lambda x: 1 if x=='successful' else 0)\n",
    "\n",
    "#5. Create new column to convert goal to USD and drop goal, rate column\n",
    "df['goal_usd'] = df['goal'] * df['static_usd_rate']\n",
    "df = df.drop(columns=['goal', 'static_usd_rate'], axis=1)\n",
    "\n",
    "#6. Convert country to full name for easier interpretation\n",
    "df['country'] = df['country'].apply(lambda x: get_country_name(x))\n",
    "\n",
    "# Pre-processing Steps added after EDA\n",
    "\n",
    "#7. Replace missing values in category column with Others (assuming these projects were not assigned a category)\n",
    "df['category'] = df['category'].fillna('Others')\n",
    "\n",
    "#8. Remove disable_communication column as it has only 1 value, so cannot contribute to prediction\n",
    "df = df.drop(columns=['disable_communication'], axis=1)\n",
    "\n",
    "#9. Keep only 1 column each for name_len and blurb_len - keeping name_len_clean and blurb_len as they have higher feature importance\n",
    "corr_cols_remove = ['name_len','blurb_len_clean']\n",
    "df = df.drop(columns=corr_cols_remove, axis=1)\n",
    "\n",
    "#10. Remove created date related columns and keep only create_to_launch_days since it captures the required information. Also, it has low feature importance scores\n",
    "created_date_cols_remove = ['created_at_weekday', 'created_at_month', 'created_at_day', 'created_at_yr', 'created_at_hr']\n",
    "df = df.drop(columns=created_date_cols_remove, axis=1)\n",
    "\n",
    " #11. Remove year columns because we are predicting for future years\n",
    "year_cols_remove = ['deadline_yr', 'launched_at_yr']\n",
    "df = df.drop(columns=year_cols_remove, axis=1)\n",
    "\n",
    "#12. Categorize countries into US and Non-US\n",
    "df['country'] = df['country'].apply(lambda x: 'US' if x=='United States' else 'Non-US')\n",
    "\n",
    "#13. Dummify categorical columns\n",
    "categorical_cols = ['country', 'category', 'deadline_weekday', 'launched_at_weekday']\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "\n",
    "X = df.drop(['state'], axis=1)\n",
    "y = df['state']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Outliers using Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessed_outliers = df.drop(columns=categorical_cols, axis=1)\n",
    "from sklearn.ensemble import IsolationForest\n",
    "isolforest = IsolationForest(contamination=0.05,random_state=0)\n",
    "pred = isolforest.fit_predict(df)\n",
    "\n",
    "from numpy import where\n",
    "anomaly_index = where(pred==-1)\n",
    "anomaly_values = df.iloc[anomaly_index]\n",
    "\n",
    "X.drop(anomaly_values.index,inplace=True, errors='ignore')\n",
    "y.drop(anomaly_values.index,inplace=True, errors='ignore')\n",
    "\n",
    "df.drop(anomaly_values.index,inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Test Split Without Standardizing Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for Standardizing Numeric Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Standardize predictors\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.6819\n",
      "Precision Score: 0.6606\n",
      "Recall Score: 0.1633\n",
      "F1 Score: 0.2618\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Creating and training the logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Calculating evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Printing the evaluation metrics\n",
    "print(f\"Accuracy Score: {accuracy:.4f}\")\n",
    "print(f\"Precision Score: {precision:.4f}\")\n",
    "print(f\"Recall Score: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.6663\n",
      "Precision Score: 0.5218\n",
      "Recall Score: 0.4070\n",
      "F1 Score: 0.4573\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Creating and training the KNN model\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = knn_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculating evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Printing the evaluation metrics\n",
    "print(f\"Accuracy Score: {accuracy:.4f}\")\n",
    "print(f\"Precision Score: {precision:.4f}\")\n",
    "print(f\"Recall Score: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_neighbors': 10, 'weights': 'uniform'}\n",
      "Accuracy Score: 0.6769\n",
      "Precision Score: 0.5643\n",
      "Recall Score: 0.2834\n",
      "F1 Score: 0.3774\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': [1, 3, 5, 7, 10],  \n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "# Create the KNN model\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "# Perform GridSearchCV to find the best parameters\n",
    "grid_search = GridSearchCV(estimator=knn_model, param_grid=param_grid, scoring='accuracy', cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters and corresponding model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set with the best model\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate evaluation metrics without specifying average for precision and recall\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the best parameters and evaluation metrics\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Accuracy Score: {accuracy:.4f}\")\n",
    "print(f\"Precision Score: {precision:.4f}\")\n",
    "print(f\"Recall Score: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.6498\n",
      "Precision Score: 0.4808\n",
      "Recall Score: 0.4640\n",
      "F1 Score: 0.4723\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Creating and training the Decision Tree model\n",
    "tree_model = DecisionTreeClassifier()\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = tree_model.predict(X_test)\n",
    "\n",
    "# Calculating evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Printing the evaluation metrics\n",
    "print(f\"Accuracy Score: {accuracy:.4f}\")\n",
    "print(f\"Precision Score: {precision:.4f}\")\n",
    "print(f\"Recall Score: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning for Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 7, 'min_samples_leaf': 4, 'min_samples_split': 5}\n",
      "Accuracy Score: 0.7039\n",
      "Precision Score: 0.5908\n",
      "Recall Score: 0.4002\n",
      "F1 Score: 0.4772\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10],  \n",
    "    'min_samples_split': [2, 5, 10],  \n",
    "    'min_samples_leaf': [1, 2, 4]  \n",
    "}\n",
    "\n",
    "# Create the Decision Tree model\n",
    "tree_model = DecisionTreeClassifier()\n",
    "\n",
    "# Perform GridSearchCV to find the best parameters\n",
    "grid_search = GridSearchCV(estimator=tree_model, param_grid=param_grid, scoring='accuracy', cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and corresponding model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set with the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the best parameters and evaluation metrics\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Accuracy Score: {accuracy:.4f}\")\n",
    "print(f\"Precision Score: {precision:.4f}\")\n",
    "print(f\"Recall Score: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.7289\n",
      "Precision Score: 0.6502\n",
      "Recall Score: 0.4269\n",
      "F1 Score: 0.5154\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "randomforest = RandomForestClassifier(random_state=123, n_estimators=150, max_features='sqrt')\n",
    "model_rf = randomforest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_rf.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy Score: {accuracy:.4f}\")\n",
    "print(f\"Precision Score: {precision:.4f}\")\n",
    "print(f\"Recall Score: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_features': 10, 'n_estimators': 150}\n",
      "Accuracy Score: 0.7278\n",
      "Precision Score: 0.6362\n",
      "Recall Score: 0.4524\n",
      "F1 Score: 0.5288\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],  \n",
    "    'max_features': [1,5,10]\n",
    "}\n",
    "\n",
    "# Create the Random Forest model\n",
    "forest_model = RandomForestClassifier()\n",
    "\n",
    "# Perform GridSearchCV to find the best parameters\n",
    "grid_search = GridSearchCV(estimator=forest_model, param_grid=param_grid, scoring='accuracy', cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and corresponding model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set with the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the best parameters and evaluation metrics\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Accuracy Score: {accuracy:.4f}\")\n",
    "print(f\"Precision Score: {precision:.4f}\")\n",
    "print(f\"Recall Score: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.7403\n",
      "Precision Score: 0.6524\n",
      "Recall Score: 0.4942\n",
      "F1 Score: 0.5624\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "gbt = GradientBoostingClassifier(random_state=123, n_estimators=400, max_features='sqrt')\n",
    "model_gbt = gbt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_gbt.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy Score: {accuracy:.4f}\")\n",
    "print(f\"Precision Score: {precision:.4f}\")\n",
    "print(f\"Recall Score: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning For Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_features': 15, 'n_estimators': 150}\n",
      "Accuracy Score: 0.7427\n",
      "Precision Score: 0.6640\n",
      "Recall Score: 0.4814\n",
      "F1 Score: 0.5582\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the GradientBoostingRegressor model\n",
    "gbt = GradientBoostingClassifier(random_state=0)\n",
    "\n",
    "# Define a grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],  # Number of boosting stages\n",
    "    'max_features': [5, 10, 15]  # Maximum number of features per tree\n",
    "}\n",
    "\n",
    "# Create the Gradient Boosting model\n",
    "gb_model = GradientBoostingClassifier()\n",
    "\n",
    "# Perform GridSearchCV to find the best parameters\n",
    "grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, scoring='accuracy', cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and corresponding model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set with the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the best parameters and evaluation metrics\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Accuracy Score: {accuracy:.4f}\")\n",
    "print(f\"Precision Score: {precision:.4f}\")\n",
    "print(f\"Recall Score: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.6835\n",
      "Precision Score: 0.5484\n",
      "Recall Score: 0.4751\n",
      "F1 Score: 0.5091\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(64, 32), momentum=0.9, random_state=0)\n",
    "\n",
    "mlp_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_test_pred = mlp_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Calculate evaluation metrics for classification\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy Score: {accuracy:.4f}\")\n",
    "print(f\"Precision Score: {precision:.4f}\")\n",
    "print(f\"Recall Score: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning for Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'hidden_layer_sizes': (64, 32), 'momentum': 0.9}\n",
      "Accuracy Score: 0.7099\n",
      "Precision Score: 0.5675\n",
      "Recall Score: 0.5473\n",
      "F1 Score: 0.5572\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(64, 32), (32, 16)],  # Change values as needed\n",
    "    'momentum': [0.9, 0.95]  # Change values as needed\n",
    "}\n",
    "\n",
    "# Create the MLPClassifier model\n",
    "mlp_classifier = MLPClassifier(random_state=0)\n",
    "\n",
    "# Perform GridSearchCV to find the best parameters\n",
    "grid_search = GridSearchCV(estimator=mlp_classifier, param_grid=param_grid, scoring='accuracy', cv=5)\n",
    "grid_search.fit(X_train_scaled_encoded, y_train)\n",
    "\n",
    "# Get the best parameters and corresponding model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set with the best model\n",
    "y_test_pred = best_model.predict(X_test_scaled_encoded)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the best parameters and evaluation metrics\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Accuracy Score: {accuracy:.4f}\")\n",
    "print(f\"Precision Score: {precision:.4f}\")\n",
    "print(f\"Recall Score: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
